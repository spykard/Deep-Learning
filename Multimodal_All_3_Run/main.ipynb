{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on CMU-Multimodal SDK\n",
    "\n",
    "This is a tutorial on using ***CMU-Multimodal SDK*** to load and process multimodal time-series datasets and training a simple late-fusion LSTM model on the processed data. \n",
    "\n",
    "For this tutorial, we specify some constants in `./constans/paths.py`. Please first take a look and modify the paths to point to the correct folders.\n",
    "\n",
    "## Downloading the data\n",
    "\n",
    "We start off by (down)loading the datasets. In the SDK each dataset has three sets of content: `highlevel`, `raw` and `labels`. `highlevel` contains the extracted features for each modality (e.g OpenFace facial landmarks, openSMILE acoustic features) while `raw` contains the raw transctripts, phonemes. `labels` are self-explanatory. Note that some datasets have more than just one set of annotations so `labels` could also give you multiple files.\n",
    "\n",
    "Currently there's a caveat that the SDK will not automatically detect if you have downloaded the data already. In event of that it will throw a `RuntimeError`. We work around that by `try/except`. This is not ideal but it will work for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'E:\\\\repo_Deep-Learning-Thesis\\\\Multimodal_All_3_SDK_Tutorials\\\\data'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-eabb74be87d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmmdataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhighlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"High-level features have been downloaded previously.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\repo_Deep-Learning-Thesis\\Multimodal_All_3_SDK\\mmsdk\\mmdatasdk\\dataset\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, recipe, destination)\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecipe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maddress\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputational_sequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomputational_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecipe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\repo_Deep-Learning-Thesis\\Multimodal_All_3_SDK\\mmsdk\\mmdatasdk\\computational_sequence\\computational_sequence.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, resource, destination, validate)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;31m#initializing the featureset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[1;31m#BACKWARD: backward compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\repo_Deep-Learning-Thesis\\Multimodal_All_3_SDK\\mmsdk\\mmdatasdk\\computational_sequence\\computational_sequence.py\u001b[0m in \u001b[0;36m__initialize\u001b[1;34m(self, resource, destination, validate)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m#reading from url or folder - main_file is where the data should go and resource is the url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__initialize_from_csd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\repo_Deep-Learning-Thesis\\Multimodal_All_3_SDK\\mmsdk\\mmdatasdk\\computational_sequence\\computational_sequence.py\u001b[0m in \u001b[0;36m__initialize_from_csd\u001b[1;34m(self, resource, destination, validate)\u001b[0m\n\u001b[0;32m     75\u001b[0m                                 \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Destination needs to be a folder where the downloaded computational sequence is stored. Exiting ...!\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                         \u001b[0mread_URL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\repo_Deep-Learning-Thesis\\Multimodal_All_3_SDK\\mmsdk\\mmdatasdk\\computational_sequence\\download_ops.py\u001b[0m in \u001b[0;36mread_URL\u001b[1;34m(url, destination)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mtotal_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'content-length'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mblock_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mwrote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'E:\\\\repo_Deep-Learning-Thesis\\\\Multimodal_All_3_SDK_Tutorials\\\\data'"
     ]
    }
   ],
   "source": [
    "from constants import SDK_PATH, DATA_PATH, WORD_EMB_PATH, CACHE_PATH\n",
    "import sys\n",
    "\n",
    "if SDK_PATH is None:\n",
    "    print(\"SDK path is not specified! Please specify first in constants/paths.py\")\n",
    "    exit(0)\n",
    "else:\n",
    "    sys.path.append(SDK_PATH)\n",
    "\n",
    "import mmsdk\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from mmsdk import mmdatasdk as md\n",
    "from subprocess import check_call, CalledProcessError\n",
    "\n",
    "# create folders for storing the data\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    check_call(' '.join(['mkdir', '-p', DATA_PATH]), shell=True)\n",
    "\n",
    "# download highlevel features, low-level (raw) data and labels for the dataset MOSI\n",
    "# if the files are already present, instead of downloading it you just load it yourself.\n",
    "# here we use CMU_MOSI dataset as example.\n",
    "\n",
    "DATASET = md.cmu_mosi\n",
    "\n",
    "try:\n",
    "    md.mmdataset(DATASET.highlevel, DATA_PATH)\n",
    "except RuntimeError:\n",
    "    print(\"High-level features have been downloaded previously.\")\n",
    "\n",
    "try:\n",
    "    md.mmdataset(DATASET.raw, DATA_PATH)\n",
    "except RuntimeError:\n",
    "    print(\"Raw data have been downloaded previously.\")\n",
    "    \n",
    "try:\n",
    "    md.mmdataset(DATASET.labels, DATA_PATH)\n",
    "except RuntimeError:\n",
    "    print(\"Labels have been downloaded previously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the downloaded files\n",
    "\n",
    "We can print the files in the target data folder to see what files are there.\n",
    "\n",
    "We can observe a bunch of files ending with `.csd` extension. This stands for ***computational sequences***, which is the underlying data structure for all features in the SDK. We will come back to that later when we load the data. For now we just print out what computational sequences we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CMU_MOSI_COVAREP.csd\nCMU_MOSI_ModifiedTimestampedWords.csd\nCMU_MOSI_OpenSmile_EB10.csd\nCMU_MOSI_openSMILE_IS09.csd\nCMU_MOSI_Opinion_Labels.csd\nCMU_MOSI_TimestampedWordVectors.csd\nCMU_MOSI_Visual_Facet_41.csd\nCMU_MOSI_Visual_Facet_42.csd\nCMU_MOSI_Visual_OpenFace_1.csd\nCMU_MOSI_Visual_OpenFace_2.csd\n"
     ]
    }
   ],
   "source": [
    "# list the directory contents... let's see what features there are\n",
    "data_files = os.listdir(DATA_PATH)\n",
    "print('\\n'.join(data_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a multimodal dataset\n",
    "\n",
    "Loading the dataset is as simple as telling the SDK what are the features you need and where are their computational sequences. You can construct a dictionary with format `{feature_name: csd_path}` and feed it to `mmdataset` object in the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "                                                                   \u001b[92m\u001b[1m[2021-05-27 08:35:04.294] | Success | \u001b[0mComputational sequence read from file E:\\repo_Deep-Learning-Thesis\\Multimodal_All_3_SDK_Tutorials\\data\\CMU_MOSI_ModifiedTimestampedWords.csd ...\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:35:04.300] | Status  | \u001b[0mChecking the integrity of the <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:35:04.300] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:35:04.321] | Success | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:35:04.321] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:35:04.321] | Warning | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2021-05-27 08:35:04.323] | Success | \u001b[0mComputational sequence read from file E:\\repo_Deep-Learning-Thesis\\Multimodal_All_3_SDK_Tutorials\\data\\CMU_MOSI_Visual_Facet_41.csd ...\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:35:04.359] | Status  | \u001b[0mChecking the integrity of the <FACET_4.1> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:35:04.359] | Status  | \u001b[0mChecking the format of the data in <FACET_4.1> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:35:04.417] | Success | \u001b[0m<FACET_4.1> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:35:04.417] | Status  | \u001b[0mChecking the format of the metadata in <FACET_4.1> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:35:04.417] | Warning | \u001b[0m<FACET_4.1> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2021-05-27 08:35:04.418] | Success | \u001b[0mComputational sequence read from file E:\\repo_Deep-Learning-Thesis\\Multimodal_All_3_SDK_Tutorials\\data\\CMU_MOSI_COVAREP.csd ...\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:35:04.430] | Status  | \u001b[0mChecking the integrity of the <COVAREP> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:35:04.431] | Status  | \u001b[0mChecking the format of the data in <COVAREP> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:35:04.475] | Success | \u001b[0m<COVAREP> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:35:04.475] | Status  | \u001b[0mChecking the format of the metadata in <COVAREP> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:35:04.475] | Warning | \u001b[0m<COVAREP> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2021-05-27 08:35:04.475] | Success | \u001b[0mDataset initialized successfully ... \n"
     ]
    }
   ],
   "source": [
    "# define your different modalities - refer to the filenames of the CSD files\n",
    "visual_field = 'CMU_MOSI_Visual_Facet_41'\n",
    "acoustic_field = 'CMU_MOSI_COVAREP'\n",
    "text_field = 'CMU_MOSI_ModifiedTimestampedWords'\n",
    "\n",
    "\n",
    "features = [\n",
    "    text_field, \n",
    "    visual_field, \n",
    "    acoustic_field\n",
    "]\n",
    "\n",
    "recipe = {feat: os.path.join(DATA_PATH, feat) + '.csd' for feat in features}\n",
    "dataset = md.mmdataset(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A peek into the dataset\n",
    "\n",
    "The multimodal dataset, after loaded, has the following hierarchy:\n",
    "\n",
    "\n",
    "```\n",
    "            computational_sequence_1 ---...\n",
    "           /                                   ...\n",
    "          /                                    /\n",
    "         /                          first_video     features -- T X N array\n",
    "        /                          /               /\n",
    "dataset ---computational_sequence_2 -- second_video\n",
    "        \\                          \\               \\\n",
    "         \\                          third_video     intervals -- T X 2 array\n",
    "          \\                                    \\...\n",
    "           \\\n",
    "            computational_sequence_3 ---...\n",
    "```\n",
    "\n",
    "It looks like a nested dictionary and can be indexed as if it is a nested dictionary. A dataset contains multiple computational sequences whose key is the `text_field`, `visual_field`, `acoustic_field` as defined above. Each computational sequence, however, has multiple video IDs in it, and different computational sequences are supposed to have the same set of video IDs. Within each video, there are two arrays: `features` and `intervals`, denoting the feature values at each time step and the start and end timestamp for each step. We can take a look at its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['CMU_MOSI_ModifiedTimestampedWords', 'CMU_MOSI_Visual_Facet_41', 'CMU_MOSI_COVAREP']\n================================================================================\n['03bSnISJMiM', '0h-zjBukYpk', '1DmNV9C1hbY', '1iG0909rllw', '2WGyTLYerpo', '2iD-tVS8NPw', '5W7Z1C_fDaE', '6Egk_28TtTM', '6_0THN4chvY', '73jzhE8R1TQ']\n================================================================================\n['features', 'intervals']\n================================================================================\n[5404, 2]\n================================================================================\n[5404, 47]\n[658, 1]\n[18009, 74]\nDifferent modalities have different number of time steps!\n"
     ]
    }
   ],
   "source": [
    "print(list(dataset.keys()))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(list(dataset[visual_field].keys())[:10])\n",
    "print(\"=\" * 80)\n",
    "\n",
    "some_id = list(dataset[visual_field].keys())[15]\n",
    "print(list(dataset[visual_field][some_id].keys()))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(list(dataset[visual_field][some_id]['intervals'].shape))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(list(dataset[visual_field][some_id]['features'].shape))\n",
    "print(list(dataset[text_field][some_id]['features'].shape))\n",
    "print(list(dataset[acoustic_field][some_id]['features'].shape))\n",
    "print(\"Different modalities have different number of time steps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment of multimodal time series\n",
    "\n",
    "To work with multimodal time series that contains multiple views of data with different frequencies, we have to first align them to a ***pivot*** modality. The convention is to align to ***words***. Alignment groups feature vectors from other modalities into bins denoted by the timestamps of the pivot modality, and apply a certain processing function to each bin. We call this function ***collapse function***, because usually it is a pooling function that collapses multiple feature vectors from another modality into one single vector. This will give you sequences of same lengths in each modality (as the length of the pivot modality) for all videos.\n",
    "\n",
    "Here we define our collapse funtion as simple averaging. We feed the function to the SDK when we invoke `align` method. Note that the SDK always expect collapse functions with two arguments: `intervals` and `features`. Even if you don't use intervals (as is in the case below) you still need to define your function in the following way.\n",
    "\n",
    "***Note: Currently the SDK applies the collapse function to all modalities including the pivot, and obviously text modality cannot be \"averaged\", causing some errors. My solution is to define the avg function such that it averages the features when it can, and return the content as is when it cannot average.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1%|██        | 149/704 [00:00<00:00, 765.54 Segments/s]\u001b[A\n",
      "Aligning etzxEpPuc6I:  33%|███▎      | 231/704 [00:00<00:00, 779.53 Segments/s]\u001b[A\n",
      "Aligning etzxEpPuc6I:  44%|████▍     | 312/704 [00:00<00:00, 787.97 Segments/s]\u001b[A\n",
      "Aligning etzxEpPuc6I:  56%|█████▌    | 394/704 [00:00<00:00, 795.78 Segments/s]\u001b[A\n",
      "Aligning etzxEpPuc6I:  68%|██████▊   | 477/704 [00:00<00:00, 804.06 Segments/s]\u001b[A\n",
      "Aligning etzxEpPuc6I:  79%|███████▉  | 559/704 [00:00<00:00, 808.39 Segments/s]\u001b[A\n",
      "Aligning etzxEpPuc6I:  91%|█████████▏| 643/704 [00:00<00:00, 817.11 Segments/s]\u001b[A\n",
      "Overall Progress:  75%|███████▌  | 70/93 [00:44<00:15,  1.49 Computational Sequence Entries/s]\n",
      "  0%|          | 0/507 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning f9O3YtZ2VfI:   0%|          | 0/507 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning f9O3YtZ2VfI:  17%|█▋        | 87/507 [00:00<00:00, 868.10 Segments/s]\u001b[A\n",
      "Aligning f9O3YtZ2VfI:  35%|███▍      | 175/507 [00:00<00:00, 871.15 Segments/s]\u001b[A\n",
      "Aligning f9O3YtZ2VfI:  52%|█████▏    | 264/507 [00:00<00:00, 876.28 Segments/s]\u001b[A\n",
      "Aligning f9O3YtZ2VfI:  69%|██████▉   | 352/507 [00:00<00:00, 875.53 Segments/s]\u001b[A\n",
      "Aligning f9O3YtZ2VfI:  87%|████████▋ | 440/507 [00:00<00:00, 876.43 Segments/s]\u001b[A\n",
      "Overall Progress:  76%|███████▋  | 71/93 [00:44<00:14,  1.55 Computational Sequence Entries/s]\n",
      "  0%|          | 0/468 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning f_pcplsH_V0:   0%|          | 0/468 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning f_pcplsH_V0:  16%|█▌        | 74/468 [00:00<00:00, 738.38 Segments/s]\u001b[A\n",
      "Aligning f_pcplsH_V0:  32%|███▏      | 152/468 [00:00<00:00, 747.90 Segments/s]\u001b[A\n",
      "Aligning f_pcplsH_V0:  50%|█████     | 234/468 [00:00<00:00, 767.69 Segments/s]\u001b[A\n",
      "Aligning f_pcplsH_V0:  67%|██████▋   | 314/468 [00:00<00:00, 775.61 Segments/s]\u001b[A\n",
      "Aligning f_pcplsH_V0:  84%|████████▎ | 391/468 [00:00<00:00, 771.10 Segments/s]\u001b[A\n",
      "Overall Progress:  77%|███████▋  | 72/93 [00:45<00:13,  1.58 Computational Sequence Entries/s]\n",
      "  0%|          | 0/666 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:   0%|          | 0/666 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:  12%|█▏        | 80/666 [00:00<00:00, 798.47 Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:  24%|██▍       | 163/666 [00:00<00:00, 806.00 Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:  37%|███▋      | 249/666 [00:00<00:00, 818.69 Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:  50%|█████     | 334/666 [00:00<00:00, 827.41 Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:  61%|██████▏   | 408/666 [00:00<00:00, 798.57 Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:  74%|███████▎  | 490/666 [00:00<00:00, 803.24 Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:  86%|████████▌ | 573/666 [00:00<00:00, 810.63 Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:  98%|█████████▊| 654/666 [00:00<00:00, 809.98 Segments/s]\u001b[A\n",
      "Overall Progress:  78%|███████▊  | 73/93 [00:46<00:13,  1.45 Computational Sequence Entries/s]\n",
      "  0%|          | 0/316 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning iiK8YX8oH1E:   0%|          | 0/316 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning iiK8YX8oH1E:  29%|██▉       | 93/316 [00:00<00:00, 923.65 Segments/s]\u001b[A\n",
      "Aligning iiK8YX8oH1E:  61%|██████    | 193/316 [00:00<00:00, 944.73 Segments/s]\u001b[A\n",
      "Aligning iiK8YX8oH1E:  93%|█████████▎| 295/316 [00:00<00:00, 964.30 Segments/s]\u001b[A\n",
      "Overall Progress:  80%|███████▉  | 74/93 [00:46<00:11,  1.72 Computational Sequence Entries/s]\n",
      "  0%|          | 0/650 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning jUzDDGyPkXU:   0%|          | 0/650 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning jUzDDGyPkXU:  13%|█▎        | 82/650 [00:00<00:00, 814.18 Segments/s]\u001b[A\n",
      "Aligning jUzDDGyPkXU:  25%|██▍       | 162/650 [00:00<00:00, 808.19 Segments/s]\u001b[A\n",
      "Aligning jUzDDGyPkXU:  38%|███▊      | 244/650 [00:00<00:00, 811.30 Segments/s]\u001b[A\n",
      "Aligning jUzDDGyPkXU:  51%|█████     | 329/650 [00:00<00:00, 820.83 Segments/s]\u001b[A\n",
      "Aligning jUzDDGyPkXU:  64%|██████▎   | 413/650 [00:00<00:00, 824.87 Segments/s]\u001b[A\n",
      "Aligning jUzDDGyPkXU:  76%|███████▋  | 497/650 [00:00<00:00, 827.61 Segments/s]\u001b[A\n",
      "Aligning jUzDDGyPkXU:  89%|████████▉ | 579/650 [00:00<00:00, 824.88 Segments/s]\u001b[A\n",
      "Overall Progress:  81%|████████  | 75/93 [00:47<00:11,  1.55 Computational Sequence Entries/s]\n",
      "  0%|          | 0/650 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning k5Y_838nuGo:   0%|          | 0/650 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning k5Y_838nuGo:  12%|█▏        | 79/650 [00:00<00:00, 717.45 Segments/s]\u001b[A\n",
      "Aligning k5Y_838nuGo:  25%|██▍       | 161/650 [00:00<00:00, 743.97 Segments/s]\u001b[A\n",
      "Aligning k5Y_838nuGo:  38%|███▊      | 245/650 [00:00<00:00, 770.04 Segments/s]\u001b[A\n",
      "Aligning k5Y_838nuGo:  50%|█████     | 328/650 [00:00<00:00, 786.67 Segments/s]\u001b[A\n",
      "Aligning k5Y_838nuGo:  62%|██████▏   | 403/650 [00:00<00:00, 773.64 Segments/s]\u001b[A\n",
      "Aligning k5Y_838nuGo:  75%|███████▍  | 487/650 [00:00<00:00, 790.89 Segments/s]\u001b[A\n",
      "Aligning k5Y_838nuGo:  88%|████████▊ | 569/650 [00:00<00:00, 797.79 Segments/s]\u001b[A\n",
      "Overall Progress:  82%|████████▏ | 76/93 [00:48<00:11,  1.44 Computational Sequence Entries/s]\n",
      "  0%|          | 0/662 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning lXPQBPVc5Cw:   0%|          | 0/662 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning lXPQBPVc5Cw:  12%|█▏        | 82/662 [00:00<00:00, 818.19 Segments/s]\u001b[A\n",
      "Aligning lXPQBPVc5Cw:  25%|██▍       | 165/662 [00:00<00:00, 820.09 Segments/s]\u001b[A\n",
      "Aligning lXPQBPVc5Cw:  37%|███▋      | 248/662 [00:00<00:00, 820.09 Segments/s]\u001b[A\n",
      "Aligning lXPQBPVc5Cw:  50%|█████     | 334/662 [00:00<00:00, 828.88 Segments/s]\u001b[A\n",
      "Aligning lXPQBPVc5Cw:  63%|██████▎   | 417/662 [00:00<00:00, 827.43 Segments/s]\u001b[A\n",
      "Aligning lXPQBPVc5Cw:  76%|███████▌  | 500/662 [00:00<00:00, 827.80 Segments/s]\u001b[A\n",
      "Aligning lXPQBPVc5Cw:  88%|████████▊ | 581/662 [00:00<00:00, 821.90 Segments/s]\u001b[A\n",
      "Overall Progress:  83%|████████▎ | 77/93 [00:49<00:11,  1.37 Computational Sequence Entries/s]\n",
      "  0%|          | 0/616 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning nbWiPyCm4g0:   0%|          | 0/616 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning nbWiPyCm4g0:  12%|█▎        | 77/616 [00:00<00:00, 768.30 Segments/s]\u001b[A\n",
      "Aligning nbWiPyCm4g0:  26%|██▌       | 159/616 [00:00<00:00, 781.58 Segments/s]\u001b[A\n",
      "Aligning nbWiPyCm4g0:  40%|███▉      | 244/616 [00:00<00:00, 799.42 Segments/s]\u001b[A\n",
      "Aligning nbWiPyCm4g0:  53%|█████▎    | 328/616 [00:00<00:00, 809.51 Segments/s]\u001b[A\n",
      "Aligning nbWiPyCm4g0:  66%|██████▋   | 409/616 [00:00<00:00, 808.05 Segments/s]\u001b[A\n",
      "Aligning nbWiPyCm4g0:  80%|███████▉  | 492/616 [00:00<00:00, 811.62 Segments/s]\u001b[A\n",
      "Aligning nbWiPyCm4g0:  94%|█████████▍| 578/616 [00:00<00:00, 822.75 Segments/s]\u001b[A\n",
      "Overall Progress:  84%|████████▍ | 78/93 [00:49<00:11,  1.36 Computational Sequence Entries/s]\n",
      "  0%|          | 0/582 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning nzpVDcQ0ywM:   0%|          | 0/582 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning nzpVDcQ0ywM:  14%|█▍        | 83/582 [00:00<00:00, 820.28 Segments/s]\u001b[A\n",
      "Aligning nzpVDcQ0ywM:  28%|██▊       | 165/582 [00:00<00:00, 819.67 Segments/s]\u001b[A\n",
      "Aligning nzpVDcQ0ywM:  43%|████▎     | 248/582 [00:00<00:00, 822.33 Segments/s]\u001b[A\n",
      "Aligning nzpVDcQ0ywM:  57%|█████▋    | 331/582 [00:00<00:00, 822.87 Segments/s]\u001b[A\n",
      "Aligning nzpVDcQ0ywM:  71%|███████   | 414/582 [00:00<00:00, 823.37 Segments/s]\u001b[A\n",
      "Aligning nzpVDcQ0ywM:  85%|████████▌ | 497/582 [00:00<00:00, 823.60 Segments/s]\u001b[A\n",
      "Aligning nzpVDcQ0ywM: 100%|█████████▉| 580/582 [00:00<00:00, 822.59 Segments/s]\u001b[A\n",
      "Overall Progress:  85%|████████▍ | 79/93 [00:50<00:10,  1.37 Computational Sequence Entries/s]\n",
      "  0%|          | 0/419 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning ob23OKe5a9Q:   0%|          | 0/419 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning ob23OKe5a9Q:  20%|██        | 85/419 [00:00<00:00, 843.99 Segments/s]\u001b[A\n",
      "Aligning ob23OKe5a9Q:  42%|████▏     | 175/419 [00:00<00:00, 858.41 Segments/s]\u001b[A\n",
      "Aligning ob23OKe5a9Q:  63%|██████▎   | 265/419 [00:00<00:00, 869.93 Segments/s]\u001b[A\n",
      "Aligning ob23OKe5a9Q:  85%|████████▍ | 356/419 [00:00<00:00, 881.08 Segments/s]\u001b[A\n",
      "Overall Progress:  86%|████████▌ | 80/93 [00:51<00:08,  1.53 Computational Sequence Entries/s]\n",
      "  0%|          | 0/502 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning pLTX3ipuDJI:   0%|          | 0/502 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning pLTX3ipuDJI:  17%|█▋        | 85/502 [00:00<00:00, 831.67 Segments/s]\u001b[A\n",
      "Aligning pLTX3ipuDJI:  36%|███▌      | 179/502 [00:00<00:00, 859.90 Segments/s]\u001b[A\n",
      "Aligning pLTX3ipuDJI:  55%|█████▍    | 274/502 [00:00<00:00, 884.54 Segments/s]\u001b[A\n",
      "Aligning pLTX3ipuDJI:  72%|███████▏  | 363/502 [00:00<00:00, 884.42 Segments/s]\u001b[A\n",
      "Aligning pLTX3ipuDJI:  91%|█████████ | 457/502 [00:00<00:00, 898.55 Segments/s]\u001b[A\n",
      "Overall Progress:  87%|████████▋ | 81/93 [00:51<00:07,  1.60 Computational Sequence Entries/s]\n",
      "  0%|          | 0/729 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:   0%|          | 0/729 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:  11%|█         | 81/729 [00:00<00:00, 804.24 Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:  23%|██▎       | 165/729 [00:00<00:00, 814.26 Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:  34%|███▍      | 249/729 [00:00<00:00, 821.29 Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:  45%|████▌     | 331/729 [00:00<00:00, 819.27 Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:  57%|█████▋    | 415/729 [00:00<00:00, 824.84 Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:  67%|██████▋   | 487/729 [00:00<00:00, 789.83 Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:  78%|███████▊  | 571/729 [00:00<00:00, 801.52 Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:  89%|████████▉ | 647/729 [00:00<00:00, 785.76 Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo: 100%|█████████▉| 727/729 [00:00<00:00, 788.31 Segments/s]\u001b[A\n",
      "Overall Progress:  88%|████████▊ | 82/93 [00:52<00:07,  1.41 Computational Sequence Entries/s]\n",
      "  0%|          | 0/716 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:   0%|          | 0/716 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:  11%|█         | 80/716 [00:00<00:00, 798.47 Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:  22%|██▏       | 160/716 [00:00<00:00, 798.46 Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:  34%|███▍      | 244/716 [00:00<00:00, 809.98 Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:  46%|████▌     | 328/716 [00:00<00:00, 817.08 Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:  57%|█████▋    | 411/716 [00:00<00:00, 818.14 Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:  69%|██████▉   | 495/716 [00:00<00:00, 822.85 Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:  80%|████████  | 573/716 [00:00<00:00, 809.08 Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:  91%|█████████▏| 655/716 [00:00<00:00, 811.87 Segments/s]\u001b[A\n",
      "Overall Progress:  89%|████████▉ | 83/93 [00:53<00:07,  1.31 Computational Sequence Entries/s]\n",
      "  0%|          | 0/599 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning tIrG4oNLFzE:   0%|          | 0/599 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning tIrG4oNLFzE:  14%|█▎        | 81/599 [00:00<00:00, 804.28 Segments/s]\u001b[A\n",
      "Aligning tIrG4oNLFzE:  27%|██▋       | 163/599 [00:00<00:00, 806.10 Segments/s]\u001b[A\n",
      "Aligning tIrG4oNLFzE:  41%|████      | 247/599 [00:00<00:00, 814.41 Segments/s]\u001b[A\n",
      "Aligning tIrG4oNLFzE:  55%|█████▌    | 330/599 [00:00<00:00, 817.29 Segments/s]\u001b[A\n",
      "Aligning tIrG4oNLFzE:  69%|██████▉   | 414/599 [00:00<00:00, 822.32 Segments/s]\u001b[A\n",
      "Aligning tIrG4oNLFzE:  83%|████████▎ | 495/599 [00:00<00:00, 816.88 Segments/s]\u001b[A\n",
      "Aligning tIrG4oNLFzE:  96%|█████████▋| 578/599 [00:00<00:00, 817.83 Segments/s]\u001b[A\n",
      "Overall Progress:  90%|█████████ | 84/93 [00:54<00:06,  1.33 Computational Sequence Entries/s]\n",
      "  0%|          | 0/432 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning tStelxIAHjw:   0%|          | 0/432 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning tStelxIAHjw:  20%|██        | 87/432 [00:00<00:00, 868.33 Segments/s]\u001b[A\n",
      "Aligning tStelxIAHjw:  41%|████▏     | 179/432 [00:00<00:00, 881.47 Segments/s]\u001b[A\n",
      "Aligning tStelxIAHjw:  63%|██████▎   | 272/432 [00:00<00:00, 894.99 Segments/s]\u001b[A\n",
      "Aligning tStelxIAHjw:  82%|████████▏ | 356/432 [00:00<00:00, 875.86 Segments/s]\u001b[A\n",
      "Overall Progress:  91%|█████████▏| 85/93 [00:54<00:05,  1.48 Computational Sequence Entries/s]\n",
      "  0%|          | 0/384 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning tmZoasNr4rU:   0%|          | 0/384 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning tmZoasNr4rU:  21%|██        | 81/384 [00:00<00:00, 808.45 Segments/s]\u001b[A\n",
      "Aligning tmZoasNr4rU:  42%|████▏     | 162/384 [00:00<00:00, 808.45 Segments/s]\u001b[A\n",
      "Aligning tmZoasNr4rU:  64%|██████▍   | 246/384 [00:00<00:00, 816.02 Segments/s]\u001b[A\n",
      "Aligning tmZoasNr4rU:  85%|████████▌ | 328/384 [00:00<00:00, 816.75 Segments/s]\u001b[A\n",
      "Overall Progress:  92%|█████████▏| 86/93 [00:55<00:04,  1.63 Computational Sequence Entries/s]\n",
      "  0%|          | 0/491 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning v0zCBqDeKcE:   0%|          | 0/491 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning v0zCBqDeKcE:  16%|█▋        | 81/491 [00:00<00:00, 808.45 Segments/s]\u001b[A\n",
      "Aligning v0zCBqDeKcE:  33%|███▎      | 163/491 [00:00<00:00, 810.23 Segments/s]\u001b[A\n",
      "Aligning v0zCBqDeKcE:  51%|█████     | 248/491 [00:00<00:00, 821.31 Segments/s]\u001b[A\n",
      "Aligning v0zCBqDeKcE:  67%|██████▋   | 328/491 [00:00<00:00, 813.03 Segments/s]\u001b[A\n",
      "Aligning v0zCBqDeKcE:  84%|████████▎ | 411/491 [00:00<00:00, 816.44 Segments/s]\u001b[A\n",
      "Overall Progress:  94%|█████████▎| 87/93 [00:55<00:03,  1.64 Computational Sequence Entries/s]\n",
      "  0%|          | 0/613 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning vvZ4IcEtiZc:   0%|          | 0/613 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning vvZ4IcEtiZc:  13%|█▎        | 81/613 [00:00<00:00, 808.45 Segments/s]\u001b[A\n",
      "Aligning vvZ4IcEtiZc:  27%|██▋       | 163/613 [00:00<00:00, 810.16 Segments/s]\u001b[A\n",
      "Aligning vvZ4IcEtiZc:  40%|████      | 246/613 [00:00<00:00, 814.42 Segments/s]\u001b[A\n",
      "Aligning vvZ4IcEtiZc:  53%|█████▎    | 327/613 [00:00<00:00, 812.62 Segments/s]\u001b[A\n",
      "Aligning vvZ4IcEtiZc:  67%|██████▋   | 408/613 [00:00<00:00, 808.95 Segments/s]\u001b[A\n",
      "Aligning vvZ4IcEtiZc:  80%|███████▉  | 490/613 [00:00<00:00, 811.77 Segments/s]\u001b[A\n",
      "Aligning vvZ4IcEtiZc:  93%|█████████▎| 573/613 [00:00<00:00, 816.69 Segments/s]\u001b[A\n",
      "Overall Progress:  95%|█████████▍| 88/93 [00:56<00:03,  1.53 Computational Sequence Entries/s]\n",
      "  0%|          | 0/600 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning vyB00TXsimI:   0%|          | 0/600 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning vyB00TXsimI:  11%|█         | 66/600 [00:00<00:00, 655.50 Segments/s]\u001b[A\n",
      "Aligning vyB00TXsimI:  24%|██▍       | 146/600 [00:00<00:00, 692.71 Segments/s]\u001b[A\n",
      "Aligning vyB00TXsimI:  38%|███▊      | 230/600 [00:00<00:00, 728.92 Segments/s]\u001b[A\n",
      "Aligning vyB00TXsimI:  52%|█████▏    | 314/600 [00:00<00:00, 758.63 Segments/s]\u001b[A\n",
      "Aligning vyB00TXsimI:  66%|██████▌   | 395/600 [00:00<00:00, 771.83 Segments/s]\u001b[A\n",
      "Aligning vyB00TXsimI:  80%|███████▉  | 477/600 [00:00<00:00, 784.06 Segments/s]\u001b[A\n",
      "Aligning vyB00TXsimI:  93%|█████████▎| 558/600 [00:00<00:00, 790.14 Segments/s]\u001b[A\n",
      "Overall Progress:  96%|█████████▌| 89/93 [00:57<00:02,  1.46 Computational Sequence Entries/s]\n",
      "  0%|          | 0/701 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:   0%|          | 0/701 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:  12%|█▏        | 81/701 [00:00<00:00, 808.23 Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:  23%|██▎       | 164/701 [00:00<00:00, 811.69 Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:  35%|███▌      | 247/701 [00:00<00:00, 815.45 Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:  46%|████▌     | 322/701 [00:00<00:00, 791.67 Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:  58%|█████▊    | 404/701 [00:00<00:00, 799.51 Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:  69%|██████▉   | 486/701 [00:00<00:00, 803.92 Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:  81%|████████  | 567/701 [00:00<00:00, 805.27 Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:  92%|█████████▏| 647/701 [00:00<00:00, 803.28 Segments/s]\u001b[A\n",
      "Overall Progress:  97%|█████████▋| 90/93 [00:58<00:02,  1.35 Computational Sequence Entries/s]\n",
      "  0%|          | 0/586 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning yDtzw_Y-7RU:   0%|          | 0/586 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning yDtzw_Y-7RU:  14%|█▍        | 81/586 [00:00<00:00, 808.23 Segments/s]\u001b[A\n",
      "Aligning yDtzw_Y-7RU:  28%|██▊       | 164/586 [00:00<00:00, 813.06 Segments/s]\u001b[A\n",
      "Aligning yDtzw_Y-7RU:  42%|████▏     | 248/586 [00:00<00:00, 819.25 Segments/s]\u001b[A\n",
      "Aligning yDtzw_Y-7RU:  57%|█████▋    | 332/586 [00:00<00:00, 824.95 Segments/s]\u001b[A\n",
      "Aligning yDtzw_Y-7RU:  70%|███████   | 412/586 [00:00<00:00, 815.59 Segments/s]\u001b[A\n",
      "Aligning yDtzw_Y-7RU:  84%|████████▍ | 492/586 [00:00<00:00, 810.38 Segments/s]\u001b[A\n",
      "Aligning yDtzw_Y-7RU:  97%|█████████▋| 569/586 [00:00<00:00, 796.05 Segments/s]\u001b[A\n",
      "Overall Progress:  98%|█████████▊| 91/93 [00:58<00:01,  1.35 Computational Sequence Entries/s]\n",
      "  0%|          | 0/625 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning yvsjCA6Y5Fc:   0%|          | 0/625 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning yvsjCA6Y5Fc:  13%|█▎        | 80/625 [00:00<00:00, 798.46 Segments/s]\u001b[A\n",
      "Aligning yvsjCA6Y5Fc:  25%|██▌       | 159/625 [00:00<00:00, 794.19 Segments/s]\u001b[A\n",
      "Aligning yvsjCA6Y5Fc:  39%|███▊      | 241/625 [00:00<00:00, 801.31 Segments/s]\u001b[A\n",
      "Aligning yvsjCA6Y5Fc:  52%|█████▏    | 323/625 [00:00<00:00, 804.08 Segments/s]\u001b[A\n",
      "Aligning yvsjCA6Y5Fc:  65%|██████▌   | 407/625 [00:00<00:00, 814.02 Segments/s]\u001b[A\n",
      "Aligning yvsjCA6Y5Fc:  78%|███████▊  | 488/625 [00:00<00:00, 812.40 Segments/s]\u001b[A\n",
      "Aligning yvsjCA6Y5Fc:  91%|█████████ | 570/625 [00:00<00:00, 812.94 Segments/s]\u001b[A\n",
      "Overall Progress:  99%|█████████▉| 92/93 [00:59<00:00,  1.33 Computational Sequence Entries/s]\n",
      "  0%|          | 0/688 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:   0%|          | 0/688 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:  11%|█         | 74/688 [00:00<00:00, 734.75 Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:  23%|██▎       | 157/688 [00:00<00:00, 758.47 Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:  35%|███▍      | 239/688 [00:00<00:00, 774.49 Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:  47%|████▋     | 321/688 [00:00<00:00, 787.10 Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:  58%|█████▊    | 402/688 [00:00<00:00, 792.24 Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:  70%|███████   | 483/688 [00:00<00:00, 795.87 Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:  82%|████████▏ | 565/688 [00:00<00:00, 801.34 Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:  94%|█████████▍| 646/688 [00:00<00:00, 803.46 Segments/s]\u001b[A\n",
      "                                                                      \u001b[92m\u001b[1m[2021-05-27 08:36:30.766] | Success | \u001b[0mAlignment to <CMU_MOSI_ModifiedTimestampedWords> complete.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:36:30.766] | Status  | \u001b[0mReplacing dataset content with aligned computational sequences\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:36:30.773] | Success | \u001b[0mInitialized empty <CMU_MOSI_ModifiedTimestampedWords> computational sequence.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:36:30.774] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:36:30.838] | Success | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:36:30.838] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:36:30.838] | Warning | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2021-05-27 08:36:30.838] | Success | \u001b[0mInitialized empty <CMU_MOSI_Visual_Facet_41> computational sequence.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:36:30.838] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_Visual_Facet_41> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:36:30.900] | Success | \u001b[0m<CMU_MOSI_Visual_Facet_41> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:36:30.900] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_Visual_Facet_41> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:36:30.900] | Warning | \u001b[0m<CMU_MOSI_Visual_Facet_41> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2021-05-27 08:36:30.900] | Success | \u001b[0mInitialized empty <CMU_MOSI_COVAREP> computational sequence.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:36:30.900] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_COVAREP> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:36:30.961] | Success | \u001b[0m<CMU_MOSI_COVAREP> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:36:30.961] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_COVAREP> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:36:30.961] | Warning | \u001b[0m<CMU_MOSI_COVAREP> computational sequence does not have all the required metadata ... continuing \n"
     ]
    }
   ],
   "source": [
    "# we define a simple averaging function that does not depend on intervals\n",
    "def avg(intervals: np.array, features: np.array) -> np.array:\n",
    "    try:\n",
    "        return np.average(features, axis=0)\n",
    "    except:\n",
    "        return features\n",
    "\n",
    "# first we align to words with averaging, collapse_function receives a list of functions\n",
    "dataset.align(text_field, collapse_functions=[avg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append annotations to the dataset and get the data points\n",
    "\n",
    "Now that we have a preprocessed dataset, all we need to do is to apply annotations to the data. Annotations are also computational sequences, since they are also just some values distributed on different time spans (e.g 1-3s is 'angry', 12-26s is 'neutral'). Hence, we just add the label computational sequence to the dataset and then align to the labels. Since we (may) want to preserve the whole sequences, this time we don't specify any collapse functions when aligning. \n",
    "\n",
    "Note that after alignment, the keys in the dataset changes from `video_id` to `video_id[segment_no]`, because alignment will segment each datapoint based on the segmentation of the pivot modality (in this case, it is segmented based on labels, which is what we need, and yes, one code block ago they are segmented to word level, which I didn't show you).\n",
    "\n",
    "***Important: DO NOT add the labels together at the beginning, the labels will be segmented during the first alignment to words. This also holds for any situation where you want to do multiple levels of alignment.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "                    \u001b[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 5W7Z1C_fDaE:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 6Egk_28TtTM:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 6_0THN4chvY:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/19 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 73jzhE8R1TQ:   0%|          | 0/19 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  11%|█         | 10/93 [00:00<00:02, 37.38 Computational Sequence Entries/s]\n",
      "  0%|          | 0/39 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 7JsX8y1ysxY:   0%|          | 0/39 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/23 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 8OtFthrtaJM:   0%|          | 0/23 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 8d-gEyoeBzc:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/26 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 8qrpnFRGt2A:   0%|          | 0/26 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  15%|█▌        | 14/93 [00:00<00:02, 36.06 Computational Sequence Entries/s]\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 9J25DZhivz8:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 9T9Hf74oK10:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 9c67fiY0wGQ:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/33 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning 9qR7uwkblbs:   0%|          | 0/33 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Af8D0E4ZXaw:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  20%|██        | 19/93 [00:00<00:02, 36.67 Computational Sequence Entries/s]\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning BI97DNYfe5I:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning BXuRRbG0Ugk:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Bfr499ggo-0:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning BioHAh1qJAQ:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  25%|██▍       | 23/93 [00:00<00:01, 36.47 Computational Sequence Entries/s]\n",
      "  0%|          | 0/26 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning BvYR0L6f2Ig:   0%|          | 0/26 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/44 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Ci-AH39fi3Y:   0%|          | 0/44 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Clx4VXItLTE:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Dg_0XKD0Mf4:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  29%|██▉       | 27/93 [00:00<00:01, 35.84 Computational Sequence Entries/s]\n",
      "  0%|          | 0/21 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning G-xst2euQUc:   0%|          | 0/21 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/29 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning G6GlGvlkxAQ:   0%|          | 0/29 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning GWuJjcEuzt8:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/34 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning HEsqda8_d0Q:   0%|          | 0/34 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  33%|███▎      | 31/93 [00:00<00:01, 36.72 Computational Sequence Entries/s]\n",
      "  0%|          | 0/39 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning I5y0__X72p0:   0%|          | 0/39 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Iu2PFX3z_1s:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning IumbAb8q2dM:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Jkswaaud0hk:   0%|          | 0/20 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  38%|███▊      | 35/93 [00:00<00:01, 37.47 Computational Sequence Entries/s]\n",
      "  0%|          | 0/29 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning LSi-o-IrDMs:   0%|          | 0/29 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning MLal-t_vJPM:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/13 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Njd1F0vZSm4:   0%|          | 0/13 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/32 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Nzq88NnDkEk:   0%|          | 0/32 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning OQvJTdtJ2H4:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  43%|████▎     | 40/93 [00:01<00:01, 39.41 Computational Sequence Entries/s]\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning OtBXNcAL_lE:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Oz06ZWiO20M:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/13 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning POKffnXeBds:   0%|          | 0/13 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning PZ-lDQFboO8:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning QN9ZIUWUXsY:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Qr1Ca94K55A:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  49%|████▉     | 46/93 [00:01<00:01, 42.48 Computational Sequence Entries/s]\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Sqr0AcuoNnk:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning TvyZBvOMOTc:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/17 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning VCslbP0mgZI:   0%|          | 0/17 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/55 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning VbQk4H8hgr0:   0%|          | 0/55 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/9 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning Vj1wYRQjB-o:   0%|          | 0/9 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  55%|█████▍    | 51/93 [00:01<00:00, 42.44 Computational Sequence Entries/s]\n",
      "  0%|          | 0/32 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning W8NXH0Djyww:   0%|          | 0/32 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning WKA5OygbEKI:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/11 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning X3j2zQgwYgE:   0%|          | 0/11 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/9 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning ZAIRrfG22O0:   0%|          | 0/9 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                  \u001b[A\n",
      "  0%|          | 0/34 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning ZUXBRvtny7o:   0%|          | 0/34 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  60%|██████    | 56/93 [00:01<00:00, 43.18 Computational Sequence Entries/s]\n",
      "  0%|          | 0/28 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning _dI--eQ6qVU:   0%|          | 0/28 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning aiEXnCPZubE:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/21 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning atnd_PF-Lbs:   0%|          | 0/21 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/34 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning bOL9jKpeJRs:   0%|          | 0/34 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning bvLlb-M3UXU:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  66%|██████▌   | 61/93 [00:01<00:00, 41.34 Computational Sequence Entries/s]\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning c5xsKMxpXnc:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/33 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning c7UH_rxdZv4:   0%|          | 0/33 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning cM3Yna7AavY:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning cW1FSBF59ik:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/29 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning cXypl4FnoZo:   0%|          | 0/29 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  71%|███████   | 66/93 [00:01<00:00, 41.03 Computational Sequence Entries/s]\n",
      "  0%|          | 0/19 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning d3_k5Xpfmik:   0%|          | 0/19 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/43 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning d6hH302o4v8:   0%|          | 0/43 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning dq3Nf_lMPnE:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/19 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning etzxEpPuc6I:   0%|          | 0/19 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning f9O3YtZ2VfI:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  76%|███████▋  | 71/93 [00:01<00:00, 40.42 Computational Sequence Entries/s]\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning f_pcplsH_V0:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning fvVhgmXxadc:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning iiK8YX8oH1E:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/27 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning jUzDDGyPkXU:   0%|          | 0/27 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning k5Y_838nuGo:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  82%|████████▏ | 76/93 [00:01<00:00, 42.11 Computational Sequence Entries/s]\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning lXPQBPVc5Cw:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning nbWiPyCm4g0:   0%|          | 0/10 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning nzpVDcQ0ywM:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning ob23OKe5a9Q:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning pLTX3ipuDJI:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/21 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning phBUpBr1hSo:   0%|          | 0/21 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  88%|████████▊ | 82/93 [00:01<00:00, 44.64 Computational Sequence Entries/s]\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning rnaNMUZpvvg:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning tIrG4oNLFzE:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning tStelxIAHjw:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning tmZoasNr4rU:   0%|          | 0/20 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning v0zCBqDeKcE:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning vvZ4IcEtiZc:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001b[A\n",
      "Overall Progress:  95%|█████████▍| 88/93 [00:02<00:00, 47.54 Computational Sequence Entries/s]\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning vyB00TXsimI:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning wMbj6ajWbic:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning yDtzw_Y-7RU:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/23 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning yvsjCA6Y5Fc:   0%|          | 0/23 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "  0%|          | 0/35 [00:00<?, ? Segments/s]\u001b[A\n",
      "Aligning zhpQhgha_KU:   0%|          | 0/35 [00:00<?, ? Segments/s]\u001b[A\n",
      "                                                                     \u001b[92m\u001b[1m[2021-05-27 08:38:01.750] | Success | \u001b[0mAlignment to <CMU_MOSI_Opinion_Labels> complete.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:38:01.750] | Status  | \u001b[0mReplacing dataset content with aligned computational sequences\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:38:01.836] | Success | \u001b[0mInitialized empty <CMU_MOSI_ModifiedTimestampedWords> computational sequence.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:38:01.836] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:38:01.840] | Success | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:38:01.840] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:38:01.840] | Warning | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2021-05-27 08:38:01.840] | Success | \u001b[0mInitialized empty <CMU_MOSI_Visual_Facet_41> computational sequence.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:38:01.840] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_Visual_Facet_41> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:38:01.844] | Success | \u001b[0m<CMU_MOSI_Visual_Facet_41> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:38:01.844] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_Visual_Facet_41> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:38:01.844] | Warning | \u001b[0m<CMU_MOSI_Visual_Facet_41> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2021-05-27 08:38:01.844] | Success | \u001b[0mInitialized empty <CMU_MOSI_COVAREP> computational sequence.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:38:01.844] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_COVAREP> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:38:01.848] | Success | \u001b[0m<CMU_MOSI_COVAREP> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:38:01.848] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_COVAREP> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:38:01.848] | Warning | \u001b[0m<CMU_MOSI_COVAREP> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2021-05-27 08:38:01.848] | Success | \u001b[0mInitialized empty <CMU_MOSI_Opinion_Labels> computational sequence.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:38:01.848] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_Opinion_Labels> computational sequence ...\n",
      "\u001b[92m\u001b[1m[2021-05-27 08:38:01.852] | Success | \u001b[0m<CMU_MOSI_Opinion_Labels> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2021-05-27 08:38:01.852] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_Opinion_Labels> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2021-05-27 08:38:01.852] | Warning | \u001b[0m<CMU_MOSI_Opinion_Labels> computational sequence does not have all the required metadata ... continuing \n"
     ]
    }
   ],
   "source": [
    "label_field = 'CMU_MOSI_Opinion_Labels'\n",
    "\n",
    "# we add and align to lables to obtain labeled segments\n",
    "# this time we don't apply collapse functions so that the temporal sequences are preserved\n",
    "label_recipe = {label_field: os.path.join(DATA_PATH, label_field + '.csd')}\n",
    "dataset.add_computational_sequences(label_recipe, destination=None)\n",
    "dataset.align(label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1iG0909rllw[3]\n"
     ]
    }
   ],
   "source": [
    "# check out what the keys look like now\n",
    "print(list(dataset[text_field].keys())[55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "\n",
    "Now it comes to our final step: splitting the dataset into train/dev/test splits. This code block is a bit long in itself, so be patience and step through carefully with the explanatory comments.\n",
    "\n",
    "The SDK provides the splits in terms of video IDs (which video belong to which split), however, after alignment our dataset keys already changed from `video_id` to `video_id[segment_no]`. Hence, we need to extract the video ID when looping through the data to determine which split each data point belongs to.\n",
    "\n",
    "In the following data processing, I also include instance-wise Z-normalization (subtract by mean and divide by standard dev) and converted words to unique IDs.\n",
    "\n",
    "This example is based on PyTorch so I am using PyTorch related utils, but the same procedure should be easy to adapt to other frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tmZoasNr4rU', 'zhpQhgha_KU', 'lXPQBPVc5Cw', 'iiK8YX8oH1E', 'tStelxIAHjw', 'nzpVDcQ0ywM', 'etzxEpPuc6I', 'cW1FSBF59ik', 'd6hH302o4v8', 'k5Y_838nuGo', 'pLTX3ipuDJI', 'jUzDDGyPkXU', 'f_pcplsH_V0', 'yvsjCA6Y5Fc', 'nbWiPyCm4g0', 'rnaNMUZpvvg', 'wMbj6ajWbic', 'cM3Yna7AavY', 'yDtzw_Y-7RU', 'vyB00TXsimI', 'dq3Nf_lMPnE', 'phBUpBr1hSo', 'd3_k5Xpfmik', 'v0zCBqDeKcE', 'tIrG4oNLFzE', 'fvVhgmXxadc', 'ob23OKe5a9Q', 'cXypl4FnoZo', 'vvZ4IcEtiZc', 'f9O3YtZ2VfI', 'c7UH_rxdZv4']\n"
     ]
    }
   ],
   "source": [
    "# obtain the train/dev/test splits - these splits are based on video IDs\n",
    "train_split = DATASET.standard_folds.standard_train_fold\n",
    "dev_split = DATASET.standard_folds.standard_valid_fold\n",
    "test_split = DATASET.standard_folds.standard_test_fold\n",
    "\n",
    "# inspect the splits: they only contain video IDs\n",
    "print(test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "E:\\Anaconda3\\envs\\python36torch\\lib\\site-packages\\ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in true_divide\n",
      "E:\\Anaconda3\\envs\\python36torch\\lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "E:\\Anaconda3\\envs\\python36torch\\lib\\site-packages\\ipykernel_launcher.py:66: RuntimeWarning: invalid value encountered in true_divide\n",
      "Total number of 0 datapoints have been dropped.\n",
      "E:\\Anaconda3\\envs\\python36torch\\lib\\site-packages\\numpy\\core\\_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "E:\\Anaconda3\\envs\\python36torch\\lib\\site-packages\\numpy\\core\\_methods.py:183: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "# we can see they are in the format of 'video_id[segment_no]', but the splits was specified with video_id only\n",
    "# we need to use regex or something to match the video IDs...\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "\n",
    "# a sentinel epsilon for safe division, without it we will replace illegal values with a constant\n",
    "EPS = 0\n",
    "\n",
    "# construct a word2id mapping that automatically takes increment when new words are encountered\n",
    "word2id = defaultdict(lambda: len(word2id))\n",
    "UNK = word2id['<unk>']\n",
    "PAD = word2id['<pad>']\n",
    "\n",
    "# place holders for the final train/dev/test dataset\n",
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "\n",
    "# define a regular expression to extract the video ID out of the keys\n",
    "pattern = re.compile('(.*)\\[.*\\]')\n",
    "num_drop = 0 # a counter to count how many data points went into some processing issues\n",
    "\n",
    "for segment in dataset[label_field].keys():\n",
    "    \n",
    "    # get the video ID and the features out of the aligned dataset\n",
    "    vid = re.search(pattern, segment).group(1)\n",
    "    label = dataset[label_field][segment]['features']\n",
    "    _words = dataset[text_field][segment]['features']\n",
    "    _visual = dataset[visual_field][segment]['features']\n",
    "    _acoustic = dataset[acoustic_field][segment]['features']\n",
    "\n",
    "    # if the sequences are not same length after alignment, there must be some problem with some modalities\n",
    "    # we should drop it or inspect the data again\n",
    "    if not _words.shape[0] == _visual.shape[0] == _acoustic.shape[0]:\n",
    "        print(f\"Encountered datapoint {vid} with text shape {_words.shape}, visual shape {_visual.shape}, acoustic shape {_acoustic.shape}\")\n",
    "        num_drop += 1\n",
    "        continue\n",
    "\n",
    "    # remove nan values\n",
    "    label = np.nan_to_num(label)\n",
    "    _visual = np.nan_to_num(_visual)\n",
    "    _acoustic = np.nan_to_num(_acoustic)\n",
    "\n",
    "    # remove speech pause tokens - this is in general helpful\n",
    "    # we should remove speech pauses and corresponding visual/acoustic features together\n",
    "    # otherwise modalities would no longer be aligned\n",
    "    words = []\n",
    "    visual = []\n",
    "    acoustic = []\n",
    "    for i, word in enumerate(_words):\n",
    "        if word[0] != b'sp':\n",
    "            words.append(word2id[word[0].decode('utf-8')]) # SDK stores strings as bytes, decode into strings here\n",
    "            visual.append(_visual[i, :])\n",
    "            acoustic.append(_acoustic[i, :])\n",
    "\n",
    "    words = np.asarray(words)\n",
    "    visual = np.asarray(visual)\n",
    "    acoustic = np.asarray(acoustic)\n",
    "\n",
    "    # z-normalization per instance and remove nan/infs\n",
    "    visual = np.nan_to_num((visual - visual.mean(0, keepdims=True)) / (EPS + np.std(visual, axis=0, keepdims=True)))\n",
    "    acoustic = np.nan_to_num((acoustic - acoustic.mean(0, keepdims=True)) / (EPS + np.std(acoustic, axis=0, keepdims=True)))\n",
    "\n",
    "    if vid in train_split:\n",
    "        train.append(((words, visual, acoustic), label, segment))\n",
    "    elif vid in dev_split:\n",
    "        dev.append(((words, visual, acoustic), label, segment))\n",
    "    elif vid in test_split:\n",
    "        test.append(((words, visual, acoustic), label, segment))\n",
    "    else:\n",
    "        print(f\"Found video that doesn't belong to any splits: {vid}\")\n",
    "\n",
    "print(f\"Total number of {num_drop} datapoints have been dropped.\")\n",
    "\n",
    "# turn off the word2id - define a named function here to allow for pickling\n",
    "def return_unk():\n",
    "    return UNK\n",
    "word2id.default_factory = return_unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the dataset\n",
    "\n",
    "Now that we have loaded the data, we can check the sizes of each split, data point shapes, vocabulary size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1283\n229\n686\n(5, 47)\n(1, 1)\n[[2.4]]\nTotal vocab size: 3143\n"
     ]
    }
   ],
   "source": [
    "# let's see the size of each set and shape of data\n",
    "print(len(train))\n",
    "print(len(dev))\n",
    "print(len(test))\n",
    "\n",
    "print(train[0][0][1].shape)\n",
    "print(train[0][1].shape)\n",
    "print(train[0][1])\n",
    "\n",
    "print(f\"Total vocab size: {len(word2id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate function in PyTorch\n",
    "\n",
    "Collate functions are functions used by PyTorch dataloader to gather batched data from dataset. It loads multiple data points from an iterable dataset object and put them in a certain format. Here we just use the lists we've constructed as the dataset and assume PyTorch dataloader will operate on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 8])\ntorch.Size([20, 8, 47])\ntorch.Size([20, 8, 74])\ntensor([[-2.6000],\n        [-1.8000],\n        [-0.8000],\n        [ 1.2000],\n        [-1.0000],\n        [-2.8000],\n        [ 0.5000],\n        [-1.6000]])\ntensor([20, 11,  9,  9,  8,  8,  6,  3])\n"
     ]
    }
   ],
   "source": [
    "def multi_collate(batch):\n",
    "    '''\n",
    "    Collate functions assume batch = [Dataset[i] for i in index_set]\n",
    "    '''\n",
    "    # for later use we sort the batch in descending order of length\n",
    "    batch = sorted(batch, key=lambda x: x[0][0].shape[0], reverse=True)\n",
    "    \n",
    "    # get the data out of the batch - use pad sequence util functions from PyTorch to pad things\n",
    "    labels = torch.cat([torch.from_numpy(sample[1]) for sample in batch], dim=0)\n",
    "    sentences = pad_sequence([torch.LongTensor(sample[0][0]) for sample in batch], padding_value=PAD)\n",
    "    visual = pad_sequence([torch.FloatTensor(sample[0][1]) for sample in batch])\n",
    "    acoustic = pad_sequence([torch.FloatTensor(sample[0][2]) for sample in batch])\n",
    "    \n",
    "    # lengths are useful later in using RNNs\n",
    "    lengths = torch.LongTensor([sample[0][0].shape[0] for sample in batch])\n",
    "    return sentences, visual, acoustic, labels, lengths\n",
    "\n",
    "# construct dataloaders, dev and test could use around ~X3 times batch size since no_grad is used during eval\n",
    "batch_sz = 56\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=batch_sz, collate_fn=multi_collate)\n",
    "dev_loader = DataLoader(dev, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)\n",
    "test_loader = DataLoader(test, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)\n",
    "\n",
    "# let's create a temporary dataloader just to see how the batch looks like\n",
    "temp_loader = iter(DataLoader(test, shuffle=True, batch_size=8, collate_fn=multi_collate))\n",
    "batch = next(temp_loader)\n",
    "\n",
    "print(batch[0].shape) # word vectors, padded to maxlen\n",
    "print(batch[1].shape) # visual features\n",
    "print(batch[2].shape) # acoustic features\n",
    "print(batch[3]) # labels\n",
    "print(batch[4]) # lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "what ill tell you who made the\n[[0.8]]\n7JsX8y1ysxY[19]\n"
     ]
    }
   ],
   "source": [
    "# Let's actually inspect the transcripts to ensure it's correct\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "examine_target = train\n",
    "idx = np.random.randint(0, len(examine_target))\n",
    "print(' '.join(list(map(lambda x: id2word[x], examine_target[idx][0][0].tolist()))))\n",
    "# print(' '.join(examine_target[idx][0]))\n",
    "print(examine_target[idx][1])\n",
    "print(examine_target[idx][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a multimodal model\n",
    "\n",
    "Here we show a simple example of late-fusion LSTM. Late-fusion refers to combining the features from different modalities at the final prediction stage, without introducing any interactions between them before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a simple model that can deal with multimodal variable length sequence\n",
    "class LFLSTM(nn.Module):\n",
    "    def __init__(self, input_sizes, hidden_sizes, fc1_size, output_size, dropout_rate):\n",
    "        super(LFLSTM, self).__init__()\n",
    "        self.input_size = input_sizes\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.fc1_size = fc1_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # defining modules - two layer bidirectional LSTM with layer norm in between\n",
    "        self.embed = nn.Embedding(len(word2id), input_sizes[0])\n",
    "        self.trnn1 = nn.LSTM(input_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "        self.trnn2 = nn.LSTM(2*hidden_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "        \n",
    "        self.vrnn1 = nn.LSTM(input_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "        self.vrnn2 = nn.LSTM(2*hidden_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "        \n",
    "        self.arnn1 = nn.LSTM(input_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "        self.arnn2 = nn.LSTM(2*hidden_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(sum(hidden_sizes)*4, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.tlayer_norm = nn.LayerNorm((hidden_sizes[0]*2,))\n",
    "        self.vlayer_norm = nn.LayerNorm((hidden_sizes[1]*2,))\n",
    "        self.alayer_norm = nn.LayerNorm((hidden_sizes[2]*2,))\n",
    "        self.bn = nn.BatchNorm1d(sum(hidden_sizes)*4)\n",
    "\n",
    "        \n",
    "    def extract_features(self, sequence, lengths, rnn1, rnn2, layer_norm):\n",
    "        packed_sequence = pack_padded_sequence(sequence, lengths)\n",
    "        packed_h1, (final_h1, _) = rnn1(packed_sequence)\n",
    "        padded_h1, _ = pad_packed_sequence(packed_h1)\n",
    "        normed_h1 = layer_norm(padded_h1)\n",
    "        packed_normed_h1 = pack_padded_sequence(normed_h1, lengths)\n",
    "        _, (final_h2, _) = rnn2(packed_normed_h1)\n",
    "        return final_h1, final_h2\n",
    "\n",
    "        \n",
    "    def fusion(self, sentences, visual, acoustic, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        sentences = self.embed(sentences)\n",
    "        \n",
    "        # extract features from text modality\n",
    "        final_h1t, final_h2t = self.extract_features(sentences, lengths, self.trnn1, self.trnn2, self.tlayer_norm)\n",
    "        \n",
    "        # extract features from visual modality\n",
    "        final_h1v, final_h2v = self.extract_features(visual, lengths, self.vrnn1, self.vrnn2, self.vlayer_norm)\n",
    "        \n",
    "        # extract features from acoustic modality\n",
    "        final_h1a, final_h2a = self.extract_features(acoustic, lengths, self.arnn1, self.arnn2, self.alayer_norm)\n",
    "\n",
    "        \n",
    "        # simple late fusion -- concatenation + normalization\n",
    "        h = torch.cat((final_h1t, final_h2t, final_h1v, final_h2v, final_h1a, final_h2a),\n",
    "                       dim=2).permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
    "        return self.bn(h)\n",
    "\n",
    "    def forward(self, sentences, visual, acoustic, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        h = self.fusion(sentences, visual, acoustic, lengths)\n",
    "        h = self.fc1(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(h)\n",
    "        o = self.fc2(h)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained embeddings\n",
    "\n",
    "We define a function for loading pretrained word embeddings stored in GloVe-style file. Contextualized embeddings obviously cannot be stored and loaded this way, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that loads data from GloVe-like embedding files\n",
    "# we will add tutorials for loading contextualized embeddings later\n",
    "# 2196017 is the vocab size of GloVe here.\n",
    "\n",
    "def load_emb(w2i, path_to_embedding, embedding_size=300, embedding_vocab=2196017, init_emb=None):\n",
    "    if init_emb is None:\n",
    "        emb_mat = np.random.randn(len(w2i), embedding_size)\n",
    "    else:\n",
    "        emb_mat = init_emb\n",
    "    f = open(path_to_embedding, 'r')\n",
    "    found = 0\n",
    "    for line in tqdm_notebook(f, total=embedding_vocab):\n",
    "        content = line.strip().split()\n",
    "        vector = np.asarray(list(map(lambda x: float(x), content[-300:])))\n",
    "        word = ' '.join(content[:-300])\n",
    "        if word in w2i:\n",
    "            idx = w2i[word]\n",
    "            emb_mat[idx, :] = vector\n",
    "            found += 1\n",
    "    print(f\"Found {found} words in the embedding file.\")\n",
    "    return torch.tensor(emb_mat).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "Next we train a model. We use Adam with gradient clipping and weight decay for training, and our loss here is Mean Absolute Error (MOSI is a regression dataset). We exclude the embeddings from trainable computation graph to prevent overfitting. We also apply a early-stopping scheme with learning rate annealing based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "E:\\Anaconda3\\envs\\python36torch\\lib\\site-packages\\ipykernel_launcher.py:51: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e39167361cc2487b85994907fac1cbda"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 1.3085\nValidation loss: 1.3867\nCurrent patience: 8, current trial: 3.\nFound new best model on dev set!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4c806b8357e4a9ba4cb464a175a90b5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.9554\nValidation loss: 1.3224\nCurrent patience: 8, current trial: 3.\nFound new best model on dev set!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bad063c00bc4676bbdb19b27f13d04f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.7299\nValidation loss: 1.3307\nCurrent patience: 8, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "088b3b4463a44b7490cc640a94287579"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.5635\nValidation loss: 1.3092\nCurrent patience: 7, current trial: 3.\nFound new best model on dev set!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddd8a5fb5d3544209ffe4956d47be688"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.4656\nValidation loss: 1.3142\nCurrent patience: 8, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c8d868548634e6994aa30655150292c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.4009\nValidation loss: 1.3452\nCurrent patience: 7, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "953c4aec12e84e2087081a88070cc774"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.3759\nValidation loss: 1.3096\nCurrent patience: 6, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b10f7b569d9942d2aedb88ba2f12bd44"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.3526\nValidation loss: 1.3273\nCurrent patience: 5, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68c9f91fccda4db1b7af2c9140b33726"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.345\nValidation loss: 1.3229\nCurrent patience: 4, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5356832ec17d4fd1b0d0810debc446f2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.3397\nValidation loss: 1.3209\nCurrent patience: 3, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "458020e3229946188c870986ef01c699"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.3263\nValidation loss: 1.3378\nCurrent patience: 2, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6d40a5eaf504c52ba217f1f26abb129"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.3139\nValidation loss: 1.2861\nCurrent patience: 1, current trial: 3.\nFound new best model on dev set!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0763c51cba4248f99135f9d25d656730"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.3041\nValidation loss: 1.3384\nCurrent patience: 8, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b96036dbd1e4b469dc8ceb02d7ea7cd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.3102\nValidation loss: 1.2917\nCurrent patience: 7, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f3f66367e334d06be07eb54df46218c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2905\nValidation loss: 1.3173\nCurrent patience: 6, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d30270a2dda4f3ca9879a101065fd58"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2984\nValidation loss: 1.3004\nCurrent patience: 5, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8900301f8c14bbcb4f4d352c43b25f2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2874\nValidation loss: 1.322\nCurrent patience: 4, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10aa5a444e7243c4bc0e54a163ab8566"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2771\nValidation loss: 1.2984\nCurrent patience: 3, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31aa1056df9d46ee853ebbdf0f2afe24"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.277\nValidation loss: 1.3239\nCurrent patience: 2, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77dd02403b634435a89fcd0d4c5a2258"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2535\nValidation loss: 1.3204\nCurrent patience: 1, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11606fb9ca12451fa33070920badda06"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2771\nValidation loss: 1.2834\nCurrent patience: 0, current trial: 3.\nFound new best model on dev set!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49d574672da749f4a203ef9125bf1a41"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2796\nValidation loss: 1.3157\nCurrent patience: 8, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc23459d4a5b4e6e8d61f60b4323be32"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2581\nValidation loss: 1.3265\nCurrent patience: 7, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d321d63a384846cfa7ac354c864a498e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.274\nValidation loss: 1.3014\nCurrent patience: 6, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90519629084d43acaee52da97bd31a9a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2691\nValidation loss: 1.3174\nCurrent patience: 5, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08ff6a4c02e94f3aaa890adb84d583ee"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.243\nValidation loss: 1.3306\nCurrent patience: 4, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cbf85bdd3ac421198e57160d5aca516"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2481\nValidation loss: 1.3083\nCurrent patience: 3, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "202acc62ac9642c4829f001cfdfabcb1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2612\nValidation loss: 1.2834\nCurrent patience: 2, current trial: 3.\nFound new best model on dev set!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1a4866cab274611999d9af02d1d06ff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.256\nValidation loss: 1.3399\nCurrent patience: 8, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a651b942c86f41fda0c554564f676f1a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2628\nValidation loss: 1.2951\nCurrent patience: 7, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9792d48a4d2d470585fdde6141d0178a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2506\nValidation loss: 1.3055\nCurrent patience: 6, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a12785b277a74ffcb37b4242f42d2788"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2366\nValidation loss: 1.3138\nCurrent patience: 5, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c065c54c5d4e4d2aa6e40e6b28a82645"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2357\nValidation loss: 1.3242\nCurrent patience: 4, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8fd4a68f8e84084ac50f7fea7982cda"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2529\nValidation loss: 1.3057\nCurrent patience: 3, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6a732fd67ce4e8bbead7234f5fbe70a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2433\nValidation loss: 1.2966\nCurrent patience: 2, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c62caeb638f4b188ed24bd7d5152b91"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2378\nValidation loss: 1.3203\nCurrent patience: 1, current trial: 3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8876915c97a149ca9313d74d72ffbced"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.23\nValidation loss: 1.3129\nCurrent patience: 0, current trial: 3.\nRunning out of patience, loading previous best model.\nCurrent learning rate: 1.0000000000000003e-05\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dad76e5118148278bc7a1d0bc4fbc0b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.257\nValidation loss: 1.2889\nCurrent patience: 8, current trial: 2.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "006f156854ec4e12b3cce319d5c19b1e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2256\nValidation loss: 1.296\nCurrent patience: 7, current trial: 2.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20142f0e45dc4e5988d03d8513993af4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2045\nValidation loss: 1.2922\nCurrent patience: 6, current trial: 2.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab42002c642c47a899facb62a27c5b80"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2074\nValidation loss: 1.3011\nCurrent patience: 5, current trial: 2.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd5423aa256846f9bcd3cff4826ca1ca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2047\nValidation loss: 1.3069\nCurrent patience: 4, current trial: 2.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff92af70c3c542fc9d0220fb7de61134"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.1791\nValidation loss: 1.302\nCurrent patience: 3, current trial: 2.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "634d1da5e073407b95c5898a75b74f58"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.1807\nValidation loss: 1.3005\nCurrent patience: 2, current trial: 2.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "815cbe1edc7740e489b7641cc45f487f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.1719\nValidation loss: 1.3064\nCurrent patience: 1, current trial: 2.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c35f821c11c449a1972bc4930cfe55fe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.173\nValidation loss: 1.3091\nCurrent patience: 0, current trial: 2.\nRunning out of patience, loading previous best model.\nCurrent learning rate: 1.0000000000000002e-06\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1753ab561336431e9e39ec92497a56dd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2446\nValidation loss: 1.2849\nCurrent patience: 8, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e03a16ed78df4d97bc5f72f0f0fcfc60"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2355\nValidation loss: 1.2837\nCurrent patience: 7, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cf141c282a740549a09c41a896bc5ba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.237\nValidation loss: 1.2872\nCurrent patience: 6, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80f09b8c711041db8c4c3c39c5b74922"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.23\nValidation loss: 1.2821\nCurrent patience: 5, current trial: 1.\nFound new best model on dev set!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c89850ff0b14a23a851e5123b3155c6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.248\nValidation loss: 1.2879\nCurrent patience: 8, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52589cae3cab465eb9617b5e73727bb1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2153\nValidation loss: 1.2879\nCurrent patience: 7, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea7b53098952480d9745e2f0ecd7788f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2262\nValidation loss: 1.2887\nCurrent patience: 6, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbd0b6a087074ad5ad48bf68dabe330c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2208\nValidation loss: 1.2872\nCurrent patience: 5, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6f9cb00a9134bc5a9258b5549a44d50"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2215\nValidation loss: 1.2924\nCurrent patience: 4, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e74bb668758442bd8f19c0d075d9731f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2224\nValidation loss: 1.2923\nCurrent patience: 3, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4b7ebbc79bc4cd194a9ce71656b8a5f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2252\nValidation loss: 1.2921\nCurrent patience: 2, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "558c14b653ab4df892a007cc001e368e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining loss: 0.2203\nValidation loss: 1.2876\nCurrent patience: 1, current trial: 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f878fa241ef746eb91a703ab2eb80660"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training loss: 0.216\n",
      "Validation loss: 1.2897\n",
      "Current patience: 0, current trial: 1.\n",
      "Running out of patience, loading previous best model.\n",
      "Current learning rate: 1.0000000000000002e-07\n",
      "Running out of patience, early stopping.\n",
      "Test set performance: 1.3316638379333319\n",
      "Test set accuracy is 0.5816326530612245\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from torch.optim import Adam, SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "MAX_EPOCH = 1000\n",
    "\n",
    "text_size = 300\n",
    "visual_size = 47\n",
    "acoustic_size = 74\n",
    "\n",
    "# define some model settings and hyper-parameters\n",
    "input_sizes = [text_size, visual_size, acoustic_size]\n",
    "hidden_sizes = [int(text_size * 1.5), int(visual_size * 1.5), int(acoustic_size * 1.5)]\n",
    "fc1_size = sum(hidden_sizes) // 2\n",
    "dropout = 0.25\n",
    "output_size = 1\n",
    "curr_patience = patience = 8\n",
    "num_trials = 3\n",
    "grad_clip_value = 1.0\n",
    "weight_decay = 0.1\n",
    "\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    pretrained_emb, word2id = torch.load(CACHE_PATH)\n",
    "elif WORD_EMB_PATH is not None:\n",
    "    pretrained_emb = load_emb(word2id, WORD_EMB_PATH)\n",
    "    torch.save((pretrained_emb, word2id), CACHE_PATH)\n",
    "else:\n",
    "    pretrained_emb = None\n",
    "\n",
    "model = LFLSTM(input_sizes, hidden_sizes, fc1_size, output_size, dropout)\n",
    "if pretrained_emb is not None:\n",
    "    model.embed.weight.data = pretrained_emb\n",
    "model.embed.requires_grad = False\n",
    "optimizer = Adam([param for param in model.parameters() if param.requires_grad], weight_decay=weight_decay)\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss(reduction='sum')\n",
    "criterion_test = nn.L1Loss(reduction='sum')\n",
    "best_valid_loss = float('inf')\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "lr_scheduler.step() # for some reason it seems the StepLR needs to be stepped once first\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "for e in range(MAX_EPOCH):\n",
    "    model.train()\n",
    "    train_iter = tqdm_notebook(train_loader)\n",
    "    train_loss = 0.0\n",
    "    for batch in train_iter:\n",
    "        model.zero_grad()\n",
    "        t, v, a, y, l = batch\n",
    "        batch_size = t.size(0)\n",
    "        if CUDA:\n",
    "            t = t.cuda()\n",
    "            v = v.cuda()\n",
    "            a = a.cuda()\n",
    "            y = y.cuda()\n",
    "            l = l.cuda()\n",
    "        y_tilde = model(t, v, a, l)\n",
    "        loss = criterion(y_tilde, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_([param for param in model.parameters() if param.requires_grad], grad_clip_value)\n",
    "        optimizer.step()\n",
    "        train_iter.set_description(f\"Epoch {e}/{MAX_EPOCH}, current batch loss: {round(loss.item()/batch_size, 4)}\")\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Training loss: {round(train_loss, 4)}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0.0\n",
    "        for batch in dev_loader:\n",
    "            model.zero_grad()\n",
    "            t, v, a, y, l = batch\n",
    "            if CUDA:\n",
    "                t = t.cuda()\n",
    "                v = v.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()\n",
    "                l = l.cuda()\n",
    "            y_tilde = model(t, v, a, l)\n",
    "            loss = criterion(y_tilde, y)\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    valid_loss = valid_loss/len(dev)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f\"Validation loss: {round(valid_loss, 4)}\")\n",
    "    print(f\"Current patience: {curr_patience}, current trial: {num_trials}.\")\n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"Found new best model on dev set!\")\n",
    "        torch.save(model.state_dict(), 'model.std')\n",
    "        torch.save(optimizer.state_dict(), 'optim.std')\n",
    "        curr_patience = patience\n",
    "    else:\n",
    "        curr_patience -= 1\n",
    "        if curr_patience <= -1:\n",
    "            print(\"Running out of patience, loading previous best model.\")\n",
    "            num_trials -= 1\n",
    "            curr_patience = patience\n",
    "            model.load_state_dict(torch.load('model.std'))\n",
    "            optimizer.load_state_dict(torch.load('optim.std'))\n",
    "            lr_scheduler.step()\n",
    "            print(f\"Current learning rate: {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "    \n",
    "    if num_trials <= 0:\n",
    "        print(\"Running out of patience, early stopping.\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('model.std'))\n",
    "y_true = []\n",
    "y_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for batch in test_loader:\n",
    "        model.zero_grad()\n",
    "        t, v, a, y, l = batch\n",
    "        if CUDA:\n",
    "            t = t.cuda()\n",
    "            v = v.cuda()\n",
    "            a = a.cuda()\n",
    "            y = y.cuda()\n",
    "            l = l.cuda()\n",
    "        y_tilde = model(t, v, a, l)\n",
    "        loss = criterion_test(y_tilde, y)\n",
    "        y_true.append(y_tilde.detach().cpu().numpy())\n",
    "        y_pred.append(y.detach().cpu().numpy())\n",
    "        test_loss += loss.item()\n",
    "print(f\"Test set performance: {test_loss/len(test)}\")\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "                  \n",
    "y_true_bin = y_true >= 0\n",
    "y_pred_bin = y_pred >= 0\n",
    "bin_acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "print(f\"Test set accuracy is {bin_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3610jvsc74a57bd05289975529581af8ff11dd7011e2b11234139165f513a43e69cdc469453260f7",
   "display_name": "Python 3.6.10 64-bit ('python36torch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}